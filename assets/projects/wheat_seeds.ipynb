{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1328417b-e800-4cc5-bb6a-3cdbb6416e98",
   "metadata": {},
   "source": [
    "# Wheat Seeds Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff66d1e7-a287-4718-86af-c80ad02f48b2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "details"
    ]
   },
   "source": [
    "## 0. Description\n",
    "\n",
    "https://archive.ics.uci.edu/datasets\n",
    "classficiation, clustering\n",
    "\n",
    "We have taken the dataset from Kaggle, data source link Wheat Seed.\n",
    "The examined group comprised kernels belonging to three different varieties of wheat: Kama, Rosa and Canadian, 70 elements each, randomly selected for the experiment. High quality visualization of the internal kernel structure was detected using a soft X-ray technique. It is non-destructive and considerably cheaper than other more sophisticated imaging techniques like scanning microscopy or laser technology. The images were recorded on 13x18 cm X-ray KODAK plates. Studies were conducted using combine harvested wheat grain originating from experimental fields, explored at the Institute of Agrophysics of the Polish Academy of Sciences in Lublin.\n",
    "The objective is to demonstrate in R to analyze all relevant wheat seed data and cluster into groups.\n",
    "\n",
    "We are importing the Wheat seed (kernels) data and performing preliminary analysis.\n",
    "\n",
    "\n",
    "seeds\n",
    "\tclassifcation and cluster analysis\n",
    "\tclassification, clustering\n",
    "\n",
    "data preparation\n",
    "data dictionnary\n",
    "\t\n",
    "EDA\n",
    "k-means clustering\n",
    "Hierarchical clustering\n",
    "Gaussian Mixture Model clustering\n",
    "\n",
    "https://rstudio-pubs-static.s3.amazonaws.com/606831_876e370c02ec44fd9e338182aca4897c.html\n",
    "\n",
    "pip install --upgrade jupyterlab-git\n",
    "\n",
    "Link https://archive.ics.uci.edu/dataset/236/seeds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0188e127-793f-4bc9-a296-453e14116eee",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Environment set up "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22465aa-9252-4f18-b215-d150e74132a4",
   "metadata": {},
   "source": [
    "### Configuration "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb4b90e-0529-41e7-98ed-c638ca11cd6c",
   "metadata": {},
   "source": [
    "### Libraries imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e93ade3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78422086-45c9-48a7-9e7b-8113ca8c4ab2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Data Dictionnary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d08da76",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "204699e9-659b-4342-a58f-7aabffe28831",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad06962c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7de8a478",
   "metadata": {},
   "source": [
    "## 3. Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5205cc3b",
   "metadata": {},
   "source": [
    "### Changing the Shape of Variables\n",
    "Many machine learning algorithm require that the variables in input should be approximately normally distributed. We already covered the normal distribution in the statistical module, so you know what that means.\n",
    "\n",
    "The reason of such assumption is often related to the symmetry of a normal distribution with can be a desired property to have. We also saw that skewness is a measure of symmetry and a perfectly symmetric distribution will have a skewness of 0.\n",
    "\n",
    "We can ask the question is there anything that we can do to change the distribution so that a variable approximately resembles a normal distribution? The answer is yes, and we can do that, by applying a transformation.\n",
    "\n",
    "Most of real-world data are right skew, like the distribution on the slide and it is very common, for example in financial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42e6af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e7924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "filename = \"./data/pima-indians-diabetes.data\"\n",
    "orig_df = read_csv(filename, sep=',', decimal='.',header = 0)\n",
    "\n",
    "#Converting zeros into NaN and replace missing values with the mean\n",
    "na_df = orig_df[['glucose','dBP','skinfold','insuline','BMI']].replace(0,np.NaN)\n",
    "na_df[['tpregnant','dpf','age','isDiabetic']] = orig_df[['tpregnant','dpf','age','isDiabetic']]\n",
    "df = na_df.fillna(na_df.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5dc278",
   "metadata": {},
   "source": [
    "### Changing Shape to Achieve 'Normality'\n",
    "Display the skewness of each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fa0201",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe15ff2a",
   "metadata": {},
   "source": [
    "**Insuline** and **dpf** are highly skewed. Maybe, we should also consider **age**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd425037",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df['dpf'],bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6e784f",
   "metadata": {},
   "source": [
    "You can see how rightskew the variable is (skew=1.92). Let's try to apply a **log transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164e5d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.displot(np.log(df['dpf']),bins=20)\n",
    "ax.set(xlabel='log(dpf)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e03842",
   "metadata": {},
   "outputs": [],
   "source": [
    "The transformation seemed to work. This 'almost' dual peak bothers me as we want the variable to be unimodal. But probably it is ok as it is not a very well-defined dual peak.  \n",
    "  \n",
    "Let's derive the new variable **log_dpf** and this will be the variable that you would like to input in any machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce10f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_dpf']=np.log(df['dpf'])\n",
    "print('Skewness of log_dpf: ', df['log_dpf'].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e1add5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09c4e909",
   "metadata": {},
   "source": [
    "### Checking Normality\n",
    "If the new variable **log_dpf** is approximately normal, than the normal probability plot (pplot) should exhibit a linear relationship. Let's verify that.  \n",
    "  To this end, we will use a **Q-Q Plot** which is a scattered plot in which the two sets of quantiles coming from a perfectly normal distribution and the series of data we want to test are plotted against each other.  \n",
    "  \n",
    "  If the two series of quantiles are coming from the same distribution (normal) then they should form an approximately straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43037373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the number of point to generate from the normal distribution\n",
    "numOfPoints = len(df['log_dpf'])\n",
    "\n",
    "# Getting the mean and standard deviation from the distribution of the transformed variable\n",
    "mean = df['log_dpf'].mean()\n",
    "sdev = df['log_dpf'].std()\n",
    "\n",
    "# Generate specified number of point from a 'True' normal distribution with the same mean and standard deviation\n",
    "ncurve = np.random.normal(mean,sdev,numOfPoints)\n",
    "\n",
    "# Generate a sample of 100 percentiles to compare\n",
    "percs = np.linspace(0,100,100)\n",
    "\n",
    "# Generate the series of quintiles using the sample\n",
    "qn_transformed = np.percentile(df['log_dpf'], percs)\n",
    "qn_normal = np.percentile(ncurve, percs)\n",
    "\n",
    "# Generate the Quintile-Quintile Plot (QQPlot)\n",
    "plt.plot(qn_transformed,qn_normal, ls=\"\", marker=\"o\")\n",
    "\n",
    "# Display the ideal line for reference\n",
    "x = np.linspace(np.min((qn_transformed.min(),qn_normal.min())), np.max((qn_transformed.max(),qn_normal.max())))\n",
    "plt.plot(x,x, color=\"k\", ls=\"--\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c09feb5",
   "metadata": {},
   "source": [
    "If the *df['log_dpf']* variable was distributed normally, all points would line up with the dotted line. However, we see that toward the two ends the points are starting to diverge, showing non-normality behavior. However, most of the points are within acceptable distance from the line and therefore, we can accept this transformation, as we don't need to be that precise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55c82d6",
   "metadata": {},
   "source": [
    "#### Comparison with the original variable\n",
    "\n",
    "Just for curiosity, let's generate the QQPlot for the original variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fcf0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the number of point to generate from the normal distribution\n",
    "numOfPoints = len(df['dpf'])\n",
    "\n",
    "# Getting the mean and standard deviation from the distribution of the transformed variable\n",
    "mean = df['dpf'].mean()\n",
    "sdev = df['dpf'].std()\n",
    "\n",
    "# Generate specified number of point from a 'True' normal distribution with the same mean and standard deviation\n",
    "ncurve = np.random.normal(mean,sdev,numOfPoints)\n",
    "\n",
    "# Generate a sample of 100 percentiles to compare\n",
    "percs = np.linspace(0,100,100)\n",
    "\n",
    "# Generate the series of quintiles using the sample\n",
    "qn_transformed = np.percentile(df['dpf'], percs)\n",
    "qn_normal = np.percentile(ncurve, percs)\n",
    "\n",
    "# Generate the Quintile-Quintile Plot (QQPlot)\n",
    "plt.plot(qn_transformed,qn_normal, ls=\"\", marker=\"o\")\n",
    "\n",
    "# Display the ideal line for reference\n",
    "x = np.linspace(np.min((qn_transformed.min(),qn_normal.min())), np.max((qn_transformed.max(),qn_normal.max())))\n",
    "plt.plot(x,x, color=\"k\", ls=\"--\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c5684c",
   "metadata": {},
   "source": [
    "We can see, how much *deviation* we have from normality in the original variable: **the transformation definitely worked!**  \n",
    "  \n",
    "  Every time you need to check for the normality assumption, either for the variable in input or for other aspects of the processing (e.g., normality assumption of  residuals and we will see this later) you can use the QQ-Plot to formaly test for that assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b4c833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e5dcd8a",
   "metadata": {},
   "source": [
    "## 3.3 Transforming Categorical to Numerical\n",
    "Some algorithms, like regression algorithms, only accept in input numerical variables. Then, we should ask the question of what to do, in this case, with categorical variables. Should we resign to the fact that we cannot use these variables?  \n",
    "  \n",
    "  The asnwer is a definite NO!  \n",
    "  \n",
    "  In this example we will use the \"Auto Imports Database\" to transform the categorical variables **body-style** into numerical flag variables.  \n",
    "  \n",
    "  Dataset Information:\n",
    "  \n",
    "   -- Creator/Donor: Jeffrey C. Schlimmer (Jeffrey.Schlimmer@a.gp.cs.cmu.edu)  \n",
    "   -- Date: 19 May 1987  \n",
    "   -- Sources:  \n",
    "     1. 1985 Model Import Car and Truck Specifications, 1985 Ward's\n",
    "        Automotive Yearbook.  \n",
    "     2. Personal Auto Manuals, Insurance Services Office, 160 Water\n",
    "        Street, New York, NY 10038  \n",
    "     3. Insurance Collision Report, Insurance Institute for Highway\n",
    "        Safety, Watergate 600, Washington, DC 20037\n",
    "        \n",
    "  Attribute names located at: https://archive.ics.uci.edu/ml/datasets/automobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265d1f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d939b000",
   "metadata": {},
   "source": [
    "**NOTE:** This dataset does not contain a header and therefore we need to specify the columns name. Luckly, the author made the list of columns readily available to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e797c073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "columns = [\"symboling\", \"norm_losses\", \"make\", \"fuel_type\", \"aspiration\",\n",
    "           \"num_doors\", \"body_style\", \"drive_wheels\", \"engine_location\",\n",
    "           \"wheel_base\", \"length\", \"width\", \"height\", \"curb_weight\",\n",
    "           \"engine_type\", \"num_cylinders\", \"engine_size\", \"fuel_system\",\n",
    "           \"bore\", \"stroke\", \"compression_ratio\", \"horsepower\", \"peak_rpm\",\n",
    "           \"city_mpg\", \"highway_mpg\", \"price\"]\n",
    "\n",
    "# Let's specify the delimiter, set the decimal point character, set the columns names and convert the '?' as NaN\n",
    "filename = \"./data/imports-85.data\"\n",
    "df = read_csv(filename, sep=',', decimal='.',names=columns, na_values=\"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66d9ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56061dd",
   "metadata": {},
   "source": [
    "Let's take a look at the variables types and display the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affd5bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76475f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1475c4e8",
   "metadata": {},
   "source": [
    "For simplicity let's focus on the **body_style** by displaying the count of unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79543c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body_style'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c8ab02",
   "metadata": {},
   "source": [
    "It seems we do not have any missing values: let's verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7594f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['body_style'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bd8612",
   "metadata": {},
   "source": [
    "For any missing value we would have replace it with the mode, which in this case was *'sedan'*; however, no missing value for this columns.  \n",
    "  \n",
    "  Pandas supports the conversion of categorical into flag or dummies variables via the **get_dummiers** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcef912",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = pd.get_dummies(df, columns=['body_style'], prefix=['style'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698a9f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25a3f26",
   "metadata": {},
   "source": [
    "You can transform as many as categorical variables at once.  \n",
    "  \n",
    "  You also noticed that Pandas create as many flag variables as the number of categories instead of k-1 as we saw in the lecture: a little bit more verbose, but it does not have any ill effects.\n",
    "  \n",
    "  **NOTE:** The body_style variables is automatically removed from the dataset's columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42dba56",
   "metadata": {},
   "source": [
    "#### Custom Mapping\n",
    "Sometimes you might want to have a simple binary encoding so that a car is either a sedan or not. In this case, you will only need of a single binary variable that take the value of **1 for sedan** and **0 otherwise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3affc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_sedan'] = np.where(df['body_style'].str.contains('sedan'),1,0)\n",
    "df[['body_style','is_sedan']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853c202d",
   "metadata": {},
   "source": [
    "Another way is to define a mapping dictionary in which we specify the exact mapping we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3505901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_doors'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a62741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doors_map={'two':2,\n",
    "           'four':4}\n",
    "\n",
    "df['num_doors'] = df['num_doors'].map(doors_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac62b1e8",
   "metadata": {},
   "source": [
    "**NOTE:** This mapping makes sense as the categorical variable has a natural ordering of its element and therefore we can simply map that information using integer values that mantain the same ordering. You can apply a similar mapping to the *num_cylinders* field for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fd8751",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3574492",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_doors'] = df['num_doors'].fillna(df['num_doors'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85edc660",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_doors'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ee0421",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.astype({'num_doors': 'int32'}).dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e56abdc",
   "metadata": {},
   "source": [
    "### Numeric to Categorical - Equal Width Binning\n",
    "If we look at the **highway_mpg** variable we have cars with a highway mileage from 16 to 54 mpg. Let's create a four bin categorical values with an equal size strategy:  \n",
    "  \n",
    "  - Low\n",
    "  - Average\n",
    "  - High\n",
    "  - Very High\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd387b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['highway_mpg'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1240cb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we have 4 bins we need 5 intervals\n",
    "categories = ['low','average','high','very high']\n",
    "\n",
    "# You can specify your own intervals, I used linspace to be more precise\n",
    "bins = np.linspace(df['highway_mpg'].min(), df['highway_mpg'].max(), len(categories)+1)\n",
    "print(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebb100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mpg_bins'] = pd.cut(df['highway_mpg'], bins=bins, labels=categories, include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de956bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's sort the value so the categories are displayed from 'low' to 'very high'\n",
    "df.sort_values('highway_mpg', inplace=True)\n",
    "plt.hist(df['mpg_bins'], bins=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ded31d",
   "metadata": {},
   "source": [
    "### Numeric to Categorical - Equal Frequency Binning\n",
    "Let's define a function that generates the label intervals and they do not have the same meaning as per the equal width case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84307bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(df, column, k=2):\n",
    "    intervals = list(set(pd.qcut(df[column], q=k, precision=1).tolist()))\n",
    "    labels = []\n",
    "    for interval in intervals:\n",
    "        sinterval = ''\n",
    "        if interval.closed_left:\n",
    "            sinterval+='['\n",
    "        else:\n",
    "            sinterval+='('\n",
    "        sinterval+=str(interval.left)+','+str(interval.right)\n",
    "        if interval.closed_right:\n",
    "            sinterval+=']'\n",
    "        else:\n",
    "            sinterval+=')'\n",
    "        labels.append(sinterval)\n",
    "    return labels\n",
    "\n",
    "categories = get_labels(df, 'highway_mpg',4)\n",
    "print(categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6500ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mpg_bins'] = pd.qcut(df['highway_mpg'], q=4, precision=1, labels=categories)\n",
    "\n",
    "# Let's sort the value so the bins intervals are displayed from the smallest to the highest\n",
    "df.sort_values('mpg_bins', inplace=True)\n",
    "ax = plt.hist(df['mpg_bins'], bins=4, edgecolor = \"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26c0a0d",
   "metadata": {},
   "source": [
    "As you can see each category as approximately the same number of records.  \n",
    "\n",
    "  As last comment, binning by the equal frequency is less common then binning by same width."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9410d8",
   "metadata": {},
   "source": [
    "### Categorical to Single Binary Variable\n",
    "Sometimes, a categorical variable has only two values and deriving two flag variables would be a waste as the information can be encoded using a single variable. By specifying *drop_first=True* Pandas will drop the first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44178be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['aspiration'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f3cee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20260b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aa587a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b24981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90f65e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b696a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e629296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e152fc80",
   "metadata": {},
   "source": [
    "## <h1> Data Preprocessing</h2>\n",
    "We cannot jump directly at mining the data without cleaning the data as we saw in the CRISP-DM methodology.\n",
    "\n",
    "Data Pre-Processing differs quite a lot when applied to numerical or textual data, so we will cover both domains separately.\n",
    "\n",
    "For numerical data you will become proficient in dealing with missing values, identifying outliers, and transforming the data to normalize its values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23268a6e",
   "metadata": {},
   "source": [
    "## <h2>Handling Missing Data</h2>\n",
    "Missing values are simply values that are not available and cells contain \"holes\". Other time missing values are in the form of encoded values (e.g. 99999) representing the absance of information.\n",
    "\n",
    "In this lab we will use the Pima Indians Diabetes Dataset.  \n",
    "  \n",
    "  **Source:** *Smith, J.W., Everhart, J.E., Dickson, W.C., Knowler, W.C., & Johannes, R.S. (1988). Using the ADAP learning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the Symposium on Computer Applications and Medical Care (pp. 261--265). IEEE Computer Society Press.*\n",
    "\n",
    "This is an interesting case, as the the author's page indicated that there were no missing values in the dataset. However, this cannot be true: there are zeros in places where they are biologically impossible, such as the blood pressure attribute. It seems very likely that zero values encode missing data. Consequently we have to use our best judgement in using this data.\n",
    "\n",
    "  Here the fields description:\n",
    "  \n",
    "  **tPregnant**: Number of times pregnant  \n",
    "  **glucose**: Plasma glucose concentration over 2 hours in an oral glucose tolerance test  \n",
    "  **dBP**: Diastolic Blood Pressure (mm Hg)  \n",
    "  **skinfold**: Triceps skin fold thickness (mm)  \n",
    "  **Insulin**: 2-Hour serum insulin (mu U/ml)  \n",
    "  **BMI**: Body mass index (weight in kg/(height in m)2)  \n",
    "  **dpf**: Diabetes pedigree function (a function which scores likelihood of diabetes based on family history)  \n",
    "  **age**: Age (years)  \n",
    "  **isDiabetic**: Class variable (0 if non-diabetic, 1 if diabetic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb95860f-d6ae-4297-a46c-080f50afce5f",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2883a2c-4c5c-4ad7-8a46-f3fdad146382",
   "metadata": {},
   "source": [
    "### Duplicate records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08cb034-827e-4d2d-b204-626b9d993be8",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c0625c",
   "metadata": {},
   "source": [
    "### l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218ca347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56369dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa707486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80814821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f9097e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875821d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e0787b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f57805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f0d849",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./data/pima-indians-diabetes.data\"\n",
    "dataset = pd.read_csv(filename, sep=',', decimal='.',header = 0)\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45c8cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting Missing values for each column\n",
    "dataset.isnull().sum()\n",
    "\n",
    "# Counting Missing values for each row\n",
    "dataset.isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fde93b9",
   "metadata": {},
   "source": [
    "If we check for null values, this dataset seems to be quite ok as we cannot find any null values!  \n",
    "\n",
    "  **NOTE:** A traditional approach to missing values is to check for *null* values. However, while this might work well in most situation, you should check for other values as well. In our case the value 0 is used to encode for *absence of information* and if you don't check your analysis might generate sub-optimal results.\n",
    "As you can see many variables have a min of 0 which does not make biological sense: e.g. glucose, diastolic BP, skinfold, insuline, BMI, etc.\n",
    "\n",
    "We have to deal with these values else, our analysis and modeling phase will be incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9725cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88cfb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Before removal: \"+str(len(na_ds)))\n",
    "na_ds_removed = na_ds.dropna()\n",
    "print (\"After removal: \"+str(len(na_ds_removed)))\n",
    "print (\"Total records removed: \"+str(len(na_ds)-len(na_ds_removed)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaaa9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98beda96",
   "metadata": {},
   "source": [
    "### <h3>Removing = records containing missing values</h3>\n",
    "As we said this is a waste and I don’t recommend it. For small number of record this could be a solution, but this is rarely the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97ca2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting zeros into NaN\n",
    "na_ds = dataset[['glucose','dBP','skinfold','insuline','BMI']].replace(0,numpy.NaN)\n",
    "na_ds[['tpregnant','dpf','age','isDiabetic']] = dataset[['tpregnant','dpf','age','isDiabetic']]\n",
    "na_ds.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9e34da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Before removal: \"+str(len(na_ds)))\n",
    "na_ds_removed = na_ds.dropna()\n",
    "print (\"After removal: \"+str(len(na_ds_removed)))\n",
    "print (\"Total records removed: \"+str(len(na_ds)-len(na_ds_removed)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2d73ec",
   "metadata": {},
   "source": [
    "We removed a total of 376 records, losing important information.\n",
    "Better to replace the missing values with other strategies.\n",
    "\n",
    "## <h3>Inputing missing values with the mean</h3>\n",
    "You can replace the missing value using the field mean, in the case of numerical value, or the mode, for categorical variables.\n",
    "\n",
    "This method works better compared to the constant method. Always keep in mind that you are fabricating data to fill the holes in the data set. Having said that, this method can work quite well and largely used.\n",
    "\n",
    "However, don’t use it if the missing value are quite numerous since you might end up with confidence intervals which could be quite over-optimistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7240bc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_mean_ds = na_ds.fillna(na_ds.mean()) \n",
    "na_mean_ds.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf394dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1861c32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a5b7092",
   "metadata": {},
   "source": [
    "We can see that insuline, skinfold are the variables affected the most by missing values, while dpf and age do not contain missing values.\n",
    "\n",
    "Sometimes, in Python, it is easier to deal with missing values if they are encoded as 'NaN' (Not a Number) as many function deal with this particular type of data automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3860d04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting zeros into NaN\n",
    "na_ds = dataset[['glucose','dBP','skinfold','insuline','BMI']].replace(0,numpy.NaN)\n",
    "na_ds[['tpregnant','dpf','age','isDiabetic']] = dataset[['tpregnant','dpf','age','isDiabetic']]\n",
    "na_ds.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbedd4f4",
   "metadata": {},
   "source": [
    "## <h3>Removing records containing missing values</h3>\n",
    "As we said this is a waste and I don’t recommend it. For small number of record this could be a solution, but this is rarely the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdd0a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Before removal: \"+str(len(na_ds)))\n",
    "na_ds_removed = na_ds.dropna()\n",
    "print (\"After removal: \"+str(len(na_ds_removed)))\n",
    "print (\"Total records removed: \"+str(len(na_ds)-len(na_ds_removed)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76a04a0",
   "metadata": {},
   "source": [
    "We removed a total of 376 records, losing important information.\n",
    "Better to replace the missing values with other strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953eda37",
   "metadata": {},
   "source": [
    "## <h3>Mean = Inputing missing values with the mean</h3>\n",
    "You can replace the missing value using the field mean, in the case of numerical value, or the mode, for categorical variables.\n",
    "\n",
    "This method works better compared to the constant method. Always keep in mind that you are fabricating data to fill the holes in the data set. Having said that, this method can work quite well and largely used.\n",
    "\n",
    "However, don’t use it if the missing value are quite numerous since you might end up with confidence intervals which could be quite over-optimistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aa802a",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_mean_ds = na_ds.fillna(na_ds.mean())\n",
    "na_mean_ds.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998720e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's confirm we don't have any missing values\n",
    "print(na_mean_ds.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8953a0",
   "metadata": {},
   "source": [
    "What is the effect on the mean and standard deviation of the original dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55f0689",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[['dBP','skinfold']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b7569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_mean_ds[['dBP','skinfold']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13b433d",
   "metadata": {},
   "source": [
    "## <h3>Random = Inputing missing values with the random values from the distribution</h3>\n",
    "This method could be superior to the mean substitution, since, the measures of center and spread should remain closer to the original. However, there is no guarantee that the produced value, or better the combination of values if you are utilizing this method for multiple fields, make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800d8ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace NaNs with random samples from the column\n",
    "def replace_nans_with_samples(column):\n",
    "    # Remove NaNs from the column\n",
    "    non_nan_values = column.dropna()\n",
    "\n",
    "    # Randomly sample from non-NaN values and replace NaNs\n",
    "    column.fillna(pd.Series(np.random.choice(non_nan_values, size=len(column.index))), inplace=True)\n",
    "\n",
    "# Apply the function to each column\n",
    "na_random_ds = na_ds.copy()  # create a copy of the dataframe\n",
    "na_random_ds.apply(lambda col: replace_nans_with_samples(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a7eb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_random_ds.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812d0bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[['dBP','skinfold']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97690394",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_random_ds[['dBP','skinfold']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eb0cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c48b70f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6104128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e02e37b",
   "metadata": {},
   "source": [
    "## Visualizaiton Outliers & detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2962e8e",
   "metadata": {},
   "source": [
    "## Part I - Visualizing Outliers\n",
    "Outliers are extreme values outside the range of what is considered normal. They do not necesserely represent error, like for example a cholesterol level of 400, which is extreme, but still a possibility. Obviously a blood pressure of 1500 is definitely an error.\n",
    "\n",
    "It is important to identify outlier because there are many statistical methods which are very sensitive to outliers which will over-influence the output. In the case of a linear regression, for example as shown in the picture, the slope of the regression line is highly affected by outliers which could completely distort the prediction of this model.\n",
    "\n",
    "### Outlier detection using histogram\n",
    "Using this method, the outliers will be at the far tails and good candidates are the ones with low frequencies and usually  disconnected from other bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc94cc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26cbb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "filename = \"./data/pima-indians-diabetes.data\"\n",
    "orig_df = read_csv(filename, sep=',', decimal='.',header = 0)\n",
    "\n",
    "#Converting zeros into NaN and replace missing values with the mean\n",
    "na_df = orig_df[['glucose','dBP','skinfold','insuline','BMI']].replace(0,np.NaN)\n",
    "na_df[['tpregnant','dpf','age','isDiabetic']] = orig_df[['tpregnant','dpf','age','isDiabetic']]\n",
    "\n",
    "df = na_df.fillna(na_df.mean())\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fca9407",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  # % magic function \n",
    "# TODO create a better visualization with the count of each bar / so we can see better the outliers \n",
    "# //! Important information\n",
    "sns.set_style(style='white')\n",
    "ax = sns.displot(df[\"skinfold\"],bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb91aea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to plot percentage with seaborn distplot / histplot / displot \n",
    "# link https://stackoverflow.com/questions/63373194/how-to-plot-percentage-with-seaborn-distplot-histplot-displot\n",
    "\n",
    "fg = sns.displot(data=data, x='age', stat='percent', col='sex', height=3.5, aspect=1.25)\n",
    "\n",
    "for ax in fg.axes.ravel():\n",
    "    \n",
    "    # add annotations\n",
    "    for c in ax.containers:\n",
    "\n",
    "        # custom label calculates percent and add an empty string so 0 value bars don't have a number\n",
    "        labels = [f'{w:0.1f}%' if (w := v.get_height()) > 0 else '' for v in c]\n",
    "\n",
    "        ax.bar_label(c, labels=labels, label_type='edge', fontsize=8, rotation=90, padding=2)\n",
    "    \n",
    "    ax.margins(y=0.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebce6953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jointplot of log_value_confirmed and log_value_deaths  \n",
    "# //! needs review \n",
    "# TODO \n",
    "sns.jointplot(x=’log_value_confirmed’,y=’log_value_deaths’,\n",
    "data=df_cases_scatter)\n",
    "#show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd371adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## section 1 \n",
    "data = np.random.default_rng(123).rayleigh(1, 70)\n",
    "counts, edges, bars = plt.hist(data)\n",
    "\n",
    "## section 2 \n",
    "fig, ax = plt.subplots()\n",
    "counts, edges, bars = ax.hist([data, data * 0.3], histtype='barstacked')\n",
    "\n",
    "for b in bars:\n",
    "    ax.bar_label(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be97288",
   "metadata": {},
   "source": [
    "https://seaborn.pydata.org/generated/seaborn.displot.html\n",
    "\n",
    "### import the necessary python packages\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# read the dataset using pandas read_csv \n",
    "# function\n",
    "data = pd.read_csv(r\"path to\\tips.csv\")\n",
    "\n",
    "# group the multi level categorical variables\n",
    "# and reset_ the index to flatten the index\n",
    "groupedvalues = data.groupby('day').sum().reset_index()\n",
    "\n",
    "# use sns barplot to plot bar plot\n",
    "# between days and tip value\n",
    "ax = sns.barplot(x='day', y='tip', \n",
    "\t\t\t\tdata=groupedvalues, \n",
    "\t\t\t\terrwidth=0)\n",
    "\n",
    "# now simply assign the bar values to\n",
    "# each bar by passing containers method\n",
    "# to bar_label function\n",
    "ax.bar_label(ax.containers[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0dfde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# // TODO\n",
    "# //! This is alert\n",
    "# TODO \n",
    "# * * Important information\n",
    "# //* This is highlight\n",
    "# //? This is query\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1858d4b9",
   "metadata": {},
   "source": [
    "### Outlier Detection using Scattered Plot\n",
    "Using a scattered plot it is possible to identify a possible outlier candidate for skinfold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bd621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.plot.scatter(x='age',y='skinfold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cc5b5b",
   "metadata": {},
   "source": [
    "### Using the Box Plot\n",
    "\n",
    "let’s turn our attention to a method that is not sensitive to outliers like the Interquartile Range (IQR).\n",
    "\n",
    "This is a very robust measure which I use quite extensively in the detection of outliers and defined as the difference between the third and first quartile, that is IQR=Q3-Q1, which represent the spread of the middle 50% of data.\n",
    "\n",
    "Any data point located 1.5 times the IQR below Q1 or 1.5 times the IQR above Q3 is considered an outlier.\n",
    "\n",
    "Let's display the distribution of the 'skinfold' or 'dBP' and the corresponding Box Plot. In the Box Plot the whiskers are drawn at:\n",
    "\n",
    "<ul>\n",
    "<li>Lower Wisker: Q1-1.5*IQR  # //! important \n",
    "<li>Upper Wisker: Q3+1.5*IQR\n",
    "</ul>\n",
    "\n",
    "***Anything below the lower wisker and about the upper wisker are outlier candidates.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97841e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "field = 'skinfold'\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.xlim(df[field].min()-1, df[field].max()*1.1)\n",
    "ax = df[field].plot(kind='kde')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.xlim(df[field].min()-1, df[field].max()*1.1)\n",
    "ax = sns.boxplot(x=df[field])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c6472d",
   "metadata": {},
   "source": [
    "You can see that the Box Plot is quite useful in identify outlier candidates.  \n",
    "  \n",
    "  I am also displaying the probability density function estimated by the KDE method and you will see how the probability associated to the points below and above the whiskets is quite low and therefore *outside the normal expected values*. Remember these are candidates outlier and it is up to us to decide what to do with them.  \n",
    "  \n",
    "  If you want to be quite aggressive, you can remove all these points from the dataset; otherwise you can just remove the most extreme values. All depends, on the type of methods you choose to adopt in resolving the business problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486f6cc",
   "metadata": {},
   "source": [
    "## Data Transformation\n",
    "Numeric variables will usually have different ranges: some small and other much larger.\n",
    "\n",
    "For example, the GPA score for a student is a variable ranging from 0 to 4, while the salary variable can assume values as large as millions of dollars.\n",
    "\n",
    "Some algorithms are very sensitive and can be greatly affected by differences in such ranges as larger values might have more influence on the output. For example, neural networks are notoriously sensitive to the range of variables and do so algorithms that use any distance measure as well.\n",
    "\n",
    "Because of this, we have to eliminate this source of distortion and bring all the variables within the same range. If the targeted range is [0,1] then we say that we are normalizing the data.\n",
    "\n",
    "Another way of controlling for this effect is using standardization which is used to standardize the scale of effect each variable has on the output.\n",
    "\n",
    "The main difference between the two is that any normalization technique will result in values within a specified interval while standardization usually does not, rather it will compare values using the same unit of measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3723b258",
   "metadata": {},
   "source": [
    "### Part I - Normalizing & Standardizing your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae5e78f",
   "metadata": {},
   "source": [
    "#### Reading the data\n",
    "By now, you should be very familiar with reading data using Pandas. Here, in addition to reading data we are replacing the missing values with the mean as we learned in the previous video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abe8299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "# Will display any number with 4 decimal points instead of the scientific notation\n",
    "pd.options.display.float_format = '{:,.4f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58481ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "filename = \"./data/pima-indians-diabetes.data\"\n",
    "orig_df = read_csv(filename, sep=',', decimal='.',header = 0)\n",
    "\n",
    "#Converting zeros into NaN and replace missing values with the mean\n",
    "na_df = orig_df[['glucose','dBP','skinfold','insuline','BMI']].replace(0,np.NaN)\n",
    "na_df[['tpregnant','dpf','age','isDiabetic']] = orig_df[['tpregnant','dpf','age','isDiabetic']]\n",
    "\n",
    "df = na_df.fillna(na_df.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14302086",
   "metadata": {},
   "source": [
    "Let's display the histogram along with the estimated probability density function related to the variables **skinfold** and **dBP**. Because there is a long tail for the skinfold distribution, we are filtering out any values larger than 60 (does it sound like an outlier filtering?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c3a760",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"'dBP' statistics: mean ({df['dBP'].mean():.4f}), std ({df['dBP'].std():.4f})\") # //! display 4 decimals points / pandas \n",
    "print(f\"'skinfold' statistics: mean ({df['skinfold'].mean():.4f}), std ({df['skinfold'].std():.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1384cf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.histplot(df[\"dBP\"],bins=20, ax=ax)\n",
    "sns.histplot(df[\"skinfold\"][df['skinfold']<60],bins=20, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8bfa32",
   "metadata": {},
   "source": [
    "As you can see the two distribution are different as **skinfold** as mean and std respectively of *29.15* and *8.79*, and **dBP** has mean of *72.40* and std of *12.10* which can be hard to compare. So let's apply some transformation and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65dbe92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2245305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "601fe52b",
   "metadata": {},
   "source": [
    "#### Min-Max Transformation\n",
    "\n",
    "Let's apply the transformation min-Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f98566",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mMskinfold']=(df['skinfold']-df['skinfold'].min())/(df['skinfold'].max()-df['skinfold'].min())\n",
    "df['mMdBP']=(df['dBP']-df['dBP'].min())/(df['dBP'].max()-df['dBP'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f55d94",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt1 = sns.histplot(df[\"mMskinfold\"],bins=20, ax=ax)\n",
    "plt2 = sns.histplot(df[\"mMdBP\"],bins=20, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61976301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f51e4caf",
   "metadata": {},
   "source": [
    "#### Z-Score Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdce87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['zskinfold'] = (df['skinfold']-df['skinfold'].mean())/df['skinfold'].std()\n",
    "df['zdBP'] = (df['dBP']-df['dBP'].mean())/df['dBP'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041b01e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.histplot(df[\"zskinfold\"][df['zskinfold']<4],bins=20, ax=ax)\n",
    "sns.histplot(df[\"zdBP\"],bins=20, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccc5c59",
   "metadata": {},
   "source": [
    "As you might have notice, that the Z-Score distribution does not change the shape of the original distribution, rather changes its unit of measure. **So, do not make the mistake to think that because you applied the Z-Score transformation the resulting variable is normally distributed: that is not correct!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98731ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89211c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c7c0d77",
   "metadata": {},
   "source": [
    "#### Decimal Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c06a0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ndigits(maxvalue):\n",
    "    return round(math.log(maxvalue,10))\n",
    "\n",
    "n=get_ndigits(df['skinfold'].abs().max())\n",
    "df['dskinfold'] = df['skinfold']/10**n\n",
    "\n",
    "n=get_ndigits(df['dBP'].abs().max())\n",
    "df['ddBP'] = df['dBP']/10**n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebe5592",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.histplot(df[\"dskinfold\"],bins=20, ax=ax)\n",
    "sns.histplot(df[\"ddBP\"],bins=20, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d16f547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eea1b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1aa67ab7",
   "metadata": {},
   "source": [
    "#### Displaying all Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ff1656",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['skinfold','mMskinfold','zskinfold','dskinfold']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44ca61f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c5cb250",
   "metadata": {},
   "source": [
    "### Part II - Numeric Method to Identify Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681bbde7",
   "metadata": {},
   "source": [
    "#### Using ZScore to identify Outliers\n",
    "\n",
    "Because a ZScore transformation changes the unit of measure, if your data is normally distributed then you also have the extra information that 99.6% of the values will be within 3 standard deviations from the mean.\n",
    "\n",
    "This means that values outside 3 standard deviations will be quite rare and far from what we should expect. In other words, these values can be considered outliers as they represent something that we would not expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0da154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's display candidate outliers with the zscore method\n",
    "coutliers = df['zskinfold'][(df['zskinfold']<-3) | (df['zskinfold']>3)]\n",
    "print(coutliers.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b65c6a",
   "metadata": {},
   "source": [
    "We have identified several candidates, let's take a further look.  \n",
    "  \n",
    "  What is the original value corresponding to 7.94528970815517?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ba5884",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = df['skinfold'].mean()\n",
    "stdev = df['skinfold'].std()\n",
    "\n",
    "print('Original_value of 7.94528970815517 is ', (7.94528970815517*stdev)+mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25018316",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['skinfold'][df['skinfold']>=99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc82721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['skinfold'].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2f0b5d",
   "metadata": {},
   "source": [
    "Ok, 99 seems to be a true outlier and should be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cc7150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(df, column):\n",
    "    mean = df[column].mean()\n",
    "    stdev = df[column].std()\n",
    "    df['z_'+column] = (df[column]-mean)/stdev\n",
    "    return df\n",
    "\n",
    "def get_zoutliers(df):\n",
    "    columns = df.columns\n",
    "    result = {}\n",
    "    for column in columns:\n",
    "        # Standardize Column\n",
    "        df = standardize(df, column)\n",
    "        coutliers = df['z_'+column][(df['z_'+column]<-3) | (df['z_'+column]>3)]\n",
    "        result[column]=coutliers.tolist()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ae2efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "coutliers = get_zoutliers(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47528d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coutliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b258bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78342574",
   "metadata": {},
   "source": [
    "#### Using Interquartile Range (IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7f2f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr_outliers(df, column):\n",
    "  q1 = df[column].quantile(0.25)\n",
    "  q3 = df[column].quantile(0.75)\n",
    "  iqr = q3 - q1\n",
    "  outliers = df[column][(df[column] < (q1 - 1.5 * iqr)) | (df[column] > (q3 + 1.5 * iqr))]\n",
    "  return outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac67a8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns\n",
    "outlier_candidates = {}\n",
    "\n",
    "for c in columns:\n",
    "  cos = iqr_outliers(df, c)\n",
    "  outlier_candidates[c] = {'n':len(cos), 'candidates':list(cos)}\n",
    "print(outlier_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02d696d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320856ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fdff10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9410b262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259eb8ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94921af7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daf019c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6270968f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79df84e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cbcf84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e88746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e5500eb-fdd7-4651-a88f-50933561d40b",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## 4. EDA (Exploration data analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bcda8e-69ba-4a2e-8fbb-3ccbe3801f3c",
   "metadata": {},
   "source": [
    "### 4.1 Correlation between Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef10ad0f-2608-41b4-87a5-e1dd9483a875",
   "metadata": {},
   "source": [
    "### 4.2 Outlier & Relationships between between target and other variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fa2821-4c7c-40eb-b5bb-dc28ae3b8832",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## 5. K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c78ab40-c951-495d-a9e2-5a257b85a2db",
   "metadata": {},
   "source": [
    "### 5.1 Finding Optimal number cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a45475a-6728-457b-b1a7-e1a1f16c5264",
   "metadata": {},
   "source": [
    "#### Elbow Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1612dfb9-57bc-4b17-a872-c146e39bdae1",
   "metadata": {},
   "source": [
    "#### Silhouette Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd89f51b-9cfb-41b8-a866-7c1f9362298b",
   "metadata": {},
   "source": [
    "#### Dunn's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a673725f-bb34-461a-aea6-1788c8b88866",
   "metadata": {},
   "source": [
    "#### Find K-means Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f771101-2bd2-4e7e-bb82-92a84a79857a",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## 6. Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0cf640-3d59-4108-af34-1afe1628147c",
   "metadata": {},
   "source": [
    "### 6.1 Finding Optimal number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5065def9-6fc4-44e4-8273-b5236e68570a",
   "metadata": {},
   "source": [
    "#### Elbow Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f576c9b7-cd3d-4b21-88fc-f5b86a5cc1af",
   "metadata": {},
   "source": [
    "#### Dunn's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6e1e7b-b080-46c1-afa4-9b2ec82e94de",
   "metadata": {},
   "source": [
    "#### Cluster Dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536a899e-2767-4606-92c5-1a327a1dc8f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63e76f8d-283b-48c6-8ef4-db3ea5faef21",
   "metadata": {},
   "source": [
    "## 7. Gaussian Mixture Model Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d293406",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03c601a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afaa8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104cdb59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b51ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113c5be4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20074cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68d933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67808b91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fed2aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4419caa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee380e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bf41fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cd8a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6c0b41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f85639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65c75a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497b51f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baff0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
