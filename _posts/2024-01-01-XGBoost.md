---
title: ' XGBoost '
author: ernest
date: 2020-11-30s 16:20:02 -05:00
last_modified_at: 2024-04-14
categories: [ Notes ]
pin:     # true
math: true
mermaid: 
published: true
tags:   # or [typography, tag-01, tag-02, etc.]
  - xgboost
  # - supervised-learning
  # - machine-learning

# image: 
  # path: /assets/sample/coming-soon.png
  # image: /assets/sample/coming-soon.png
  # alt: Responsive rendering of Chirpy theme on multiple devices.

---




### Summary

Tree boosting system that is sparsity-aware and performs weighted approximated tree learning. XGBoost is very scalable and includes automatics feature selection. 



### Inputs

  - Numerical
  - Categorical

### Preprocessing

  - Data needs to be in a DMatrix
  


<!-- 

After you drag a Classification tool onto the canvas, you can select the XGBoost option in the configuration window to solve a binary or multiclass classification problem.

In this example, you predict what species an iris flower is, based on four features: petal length, petal width, sepal length, and sepal width. Select Run to make predictions.

Select the Browse tool that is connected to the Predict tool to see the predictions. In the results grid, the column “Species_predicted” contains predictions for each row in the dataset. Compare the “Species_predicted” column with the “Species” column to see how well the model predicts what species an iris flower is.

Like the random-forest algorithm, XGBoost randomly samples the data and then creates decision trees for all of those samples. Unlike the random-forest algorithm, however, it creates decision trees sequentially and allows newer decision trees to learn from the errors of older ones. In this way, the algorithm prunes decision trees that do not perform well and leaves behind only decision trees that best predict the target.

-->

We can use the XGBoost algorithm for binary or multiclass classification. The algorithm performs best when making predictions about features and a target that have non-linear associations. The algorithm requires more processing power than others, such as logistic regression and decision tree.




XGBoost, short for eXtreme Gradient Boosting, is a powerful machine learning algorithm renowned for its efficiency and performance in supervised learning tasks, particularly in regression and classification problems. Here's a breakdown of how it works:

1. **Boosting Ensemble Method**: 
  
  XGBoost belongs to the family of ensemble learning methods, where multiple models are combined to produce a stronger predictive model. Unlike bagging methods like Random Forest, which build multiple models independently, boosting methods train models sequentially, with each new model focusing on the weaknesses of the previous ones.

2. **Gradient Boosting Framework**: 

  XGBoost is built on the gradient boosting framework. It starts by creating a simple model as the initial approximation to the target variable. Then, it iteratively builds additional models, each one correcting the errors of its predecessors. It does this by fitting each new model to the residuals (the differences between predicted and actual values) of the previous model.

3. **Optimization Objective**: 

  XGBoost optimizes a specific objective function while adding new models to the ensemble. This objective function consists of two main components: a loss function that measures the difference between predicted and actual values, and a regularization term that penalizes the complexity of the model to prevent overfitting.

4. **Tree-Based Models**: 
  
  XGBoost primarily uses decision trees as base learners. However, it differs from traditional decision trees by incorporating various optimization techniques to enhance their performance, such as pruning, column subsampling, and weighted quantile sketching.

5. **Parallel and Distributed Computing**: 

  XGBoost is designed for scalability and efficiency. It supports parallel and distributed computing, making it suitable for large datasets and high-dimensional feature spaces.

Overall, XGBoost's combination of boosting, gradient optimization, and advanced tree-based models results in a highly accurate and versatile algorithm that excels in a wide range of machine learning tasks.





